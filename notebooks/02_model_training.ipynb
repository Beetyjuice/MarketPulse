{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ Entra√Ænement des Mod√®les ML - MarketPulse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectif\n",
    "\n",
    "Ce notebook entra√Æne les mod√®les ML pour la pr√©diction des prix (LSTM) et l'analyse de sentiment (FinBERT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Configuration de l'affichage\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 1: Entra√Ænement du mod√®le LSTM pour la pr√©diction des prix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel:\n",
    "    def __init__(self, sequence_length=60):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.model = None\n",
    "        self.scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        \n",
    "    def prepare_data(self, data, feature_columns=['Close']):\n",
    "        \"\"\"Pr√©parer les donn√©es pour l'entra√Ænement LSTM\"\"\"\n",
    "        # S√©lectionner les caract√©ristiques\n",
    "        df = data[feature_columns].copy()\n",
    "        \n",
    "        # Normaliser les donn√©es\n",
    "        scaled_data = self.scaler.fit_transform(df)\n",
    "        \n",
    "        # Cr√©er des s√©quences\n",
    "        X, y = [], []\n",
    "        for i in range(self.sequence_length, len(scaled_data)):\n",
    "            X.append(scaled_data[i-self.sequence_length:i])\n",
    "            y.append(scaled_data[i, 0])  # Utiliser le prix de cl√¥ture comme cible\n",
    "        \n",
    "        X, y = np.array(X), np.array(y)\n",
    "        \n",
    "        # Diviser en ensembles d'entra√Ænement et de test\n",
    "        split_idx = int(0.8 * len(X))\n",
    "        X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "        y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    def build_model(self):\n",
    "        \"\"\"Construire le mod√®le LSTM\"\"\"\n",
    "        model = Sequential([\n",
    "            LSTM(50, return_sequences=True, input_shape=(self.sequence_length, 1)),\n",
    "            Dropout(0.2),\n",
    "            LSTM(50, return_sequences=True),\n",
    "            Dropout(0.2),\n",
    "            LSTM(50),\n",
    "            Dropout(0.2),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        \n",
    "        model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "        return model\n",
    "    \n",
    "    def train(self, X_train, y_train, epochs=50, batch_size=32):\n",
    "        \"\"\"Entra√Æner le mod√®le LSTM\"\"\"\n",
    "        self.model = self.build_model()\n",
    "        \n",
    "        # Entra√Æner le mod√®le\n",
    "        history = self.model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_split=0.1,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Faire des pr√©dictions\"\"\"\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def evaluate(self, X_test, y_test):\n",
    "        \"\"\"√âvaluer le mod√®le\"\"\"\n",
    "        predictions = self.predict(X_test)\n",
    "        \n",
    "        # Inverser la transformation pour les pr√©dictions et valeurs r√©elles\n",
    "        pred_actual = self.scaler.inverse_transform(\n",
    "            np.concatenate([predictions, np.zeros((predictions.shape[0], 4))], axis=1)\n",
    "        )[:, 0]\n",
    "        y_test_actual = self.scaler.inverse_transform(\n",
    "            np.concatenate([y_test.reshape(-1, 1), np.zeros((y_test.shape[0], 4))], axis=1)\n",
    "        )[:, 0]\n",
    "        \n",
    "        mse = mean_squared_error(y_test_actual, pred_actual)\n",
    "        mae = mean_absolute_error(y_test_actual, pred_actual)\n",
    "        \n",
    "        return mse, mae, pred_actual, y_test_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les donn√©es ML pr√©par√©es\n",
    "symbol = \"AAPL\"\n",
    "data = pd.read_csv(f'data/processed/{symbol}_ml_data.csv')\n",
    "print(f\"Donn√©es charg√©es: {data.shape}\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entra√Æner le mod√®le LSTM\n",
    "lstm_model = LSTMModel(sequence_length=60)\n",
    "\n",
    "# Pr√©parer les donn√©es\n",
    "X_train, X_test, y_train, y_test = lstm_model.prepare_data(data[['Close']])\n",
    "\n",
    "# Reshape pour LSTM (samples, time steps, features)\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entra√Æner le mod√®le\n",
    "history = lstm_model.train(X_train, y_train, epochs=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âvaluer le mod√®le\n",
    "mse, mae, pred_actual, y_test_actual = lstm_model.evaluate(X_test, y_test)\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"MAE: {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser les r√©sultats\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.plot(y_test_actual, label='Valeurs r√©elles', color='blue')\n",
    "plt.plot(pred_actual, label='Pr√©dictions', color='red')\n",
    "plt.title(f'Pr√©dictions vs R√©alit√© - {symbol}')\n",
    "plt.xlabel('Temps')\n",
    "plt.ylabel('Prix')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le mod√®le\n",
    "model_dir = \"models\"\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "    \n",
    "lstm_model.model.save(f\"{model_dir}/lstm_model_{symbol}.h5\")\n",
    "print(f\"Mod√®le LSTM sauvegard√©: {model_dir}/lstm_model_{symbol}.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 2: Entra√Ænement du mod√®le FinBERT pour l'analyse de sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentModel:\n",
    "    def __init__(self, model_name=\"ProsusAI/finbert\"):\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "        self.classifier = None\n",
    "        \n",
    "    def create_sample_data(self):\n",
    "        \"\"\"Cr√©er des donn√©es d'exemple pour l'entra√Ænement\"\"\"\n",
    "        # Dans un sc√©nario r√©el, vous chargeriez des donn√©es r√©elles\n",
    "        # Pour cet exemple, nous cr√©ons des donn√©es synth√©tiques\n",
    "        texts = [\n",
    "            \"The company reported strong quarterly earnings, exceeding analyst expectations.\",\n",
    "            \"Market volatility increases as trade tensions escalate between major economies.\",\n",
    "            \"New regulatory changes could impact the financial sector significantly.\",\n",
    "            \"Stock prices surge following positive FDA approval for new drug.\",\n",
    "            \"Economic indicators suggest a potential slowdown in the coming quarters.\",\n",
    "            \"Company announces major acquisition that could transform its market position.\",\n",
    "            \"Investors show caution amid uncertainty about future economic policies.\",\n",
    "            \"Technology sector shows robust growth with new innovation breakthroughs.\",\n",
    "            \"Oil prices drop due to oversupply concerns in the global market.\",\n",
    "            \"Consumer spending increases, indicating strong economic confidence.\"\n",
    "        ]\n",
    "        \n",
    "        # Labels: 0=n√©gatif, 1=neutre, 2=positif\n",
    "        labels = [2, 0, 1, 2, 0, 2, 1, 2, 0, 2]\n",
    "        \n",
    "        return pd.DataFrame({\"text\": texts, \"label\": labels})\n",
    "    \n",
    "    def tokenize_data(self, texts):\n",
    "        \"\"\"Tokeniser les textes\"\"\"\n",
    "        return self.tokenizer(\n",
    "            texts.tolist(),\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "    \n",
    "    def train(self, data, output_dir=\"./finbert_sentiment\", epochs=3, batch_size=8):\n",
    "        \"\"\"Entra√Æner le mod√®le de sentiment\"\"\"\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "        \n",
    "        # Diviser les donn√©es\n",
    "        train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "            data[\"text\"], data[\"label\"], test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Tokeniser les donn√©es\n",
    "        train_encodings = self.tokenize_data(train_texts)\n",
    "        val_encodings = self.tokenize_data(val_texts)\n",
    "        \n",
    "        # Cr√©er une classe de dataset simple\n",
    "        class FinancialNewsDataset(torch.utils.data.Dataset):\n",
    "            def __init__(self, encodings, labels):\n",
    "                self.encodings = encodings\n",
    "                self.labels = labels\n",
    "\n",
    "            def __getitem__(self, idx):\n",
    "                item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "                item['labels'] = torch.tensor(self.labels.iloc[idx])\n",
    "                return item\n",
    "\n",
    "            def __len__(self):\n",
    "                return len(self.labels)\n",
    "        \n",
    "        # Cr√©er les datasets\n",
    "        train_dataset = FinancialNewsDataset(train_encodings, train_labels.reset_index(drop=True))\n",
    "        val_dataset = FinancialNewsDataset(val_encodings, val_labels.reset_index(drop=True))\n",
    "        \n",
    "        # D√©finir les arguments d'entra√Ænement\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            num_train_epochs=epochs,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            warmup_steps=500,\n",
    "            weight_decay=0.01,\n",
    "            logging_dir='./logs',\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            load_best_model_at_end=True,\n",
    "        )\n",
    "        \n",
    "        # Cr√©er le trainer\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "        )\n",
    "        \n",
    "        # Entra√Æner le mod√®le\n",
    "        trainer.train()\n",
    "        \n",
    "        # Sauvegarder le mod√®le\n",
    "        model_dir = \"models/finbert_sentiment\"\n",
    "        if not os.path.exists(model_dir):\n",
    "            os.makedirs(model_dir)\n",
    "        \n",
    "        trainer.save_model(model_dir)\n",
    "        self.tokenizer.save_pretrained(model_dir)\n",
    "        \n",
    "        # Cr√©er le classifieur pour l'inf√©rence\n",
    "        self.classifier = pipeline(\n",
    "            \"sentiment-analysis\",\n",
    "            model=\"models/finbert_sentiment\",\n",
    "            tokenizer=\"models/finbert_sentiment\"\n",
    "        )\n",
    "        \n",
    "        return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entra√Æner le mod√®le de sentiment\n",
    "sentiment_model = SentimentModel()\n",
    "data = sentiment_model.create_sample_data()\n",
    "\n",
    "print(\"Donn√©es d'entra√Ænement:\")\n",
    "print(data)\n",
    "\n",
    "# Entra√Æner le mod√®le (dans un environnement r√©el avec suffisamment de ressources)\n",
    "# trainer = sentiment_model.train(data)\n",
    "\n",
    "# Pour cet exemple, nous utiliserons le mod√®le pr√©-entra√Æn√©\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"ProsusAI/finbert\")\n",
    "\n",
    "# Tester le mod√®le\n",
    "test_texts = [\n",
    "    \"The company's earnings exceeded expectations, showing strong growth.\",\n",
    "    \"Market uncertainty continues to affect investor confidence.\",\n",
    "    \"New product launch expected to drive revenue growth.\"\n",
    "]\n",
    "\n",
    "print(\"\\nR√©sultats du mod√®le de sentiment:\")\n",
    "for text in test_texts:\n",
    "    result = classifier(text)\n",
    "    print(f\"Texte: {text}\")\n",
    "    print(f\"Sentiment: {result}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Ce notebook a entra√Æn√© deux mod√®les ML importants pour MarketPulse:\n",
    "\n",
    "1. **Mod√®le LSTM** pour pr√©dire les prix des actions\n",
    "2. **Mod√®le FinBERT** pour l'analyse de sentiment\n",
    "\n",
    "Les mod√®les sont maintenant pr√™ts √† √™tre int√©gr√©s dans le pipeline de traitement de donn√©es."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}